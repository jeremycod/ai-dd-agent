# AI Model Interaction Guidelines

Guidelines for effective and responsible interaction with Anthropic Claude and OpenAI GPT models.

## 1. Prompt Engineering
- **Clear Instructions:** Provide clear, concise, and unambiguous instructions in prompts.
- **Role Definition:** Clearly define the AI's role and persona (e.g., "You are an expert diagnostic assistant...").
- **Constraints & Guardrails:** Explicitly state any constraints, limitations, or safety guidelines. (e.g., "Do not ask for customer PII," "Only provide information from validated sources.")
- **Contextual Information:** Include all necessary context in the prompt (e.g., extracted entities, collected data snippets, historical context from memory retrieval).
- **Few-Shot Examples:** Where appropriate, provide few-shot examples of desired input/output formats or behaviors.
- **Structured Output Request:** Request specific output formats (e.g., "Respond in JSON format with fields `problemSummary` and `actionableSteps`," "Use Markdown headings for clarity.")

## 2. Model Selection
- **Task-Specific Models:** Choose the appropriate model (Claude, GPT) based on the specific task (e.g., Claude for complex reasoning/summarization, GPT for specific data extraction if benchmarked as better).
- **Cost-Effectiveness:** Consider the cost implications of different models and adjust usage for less critical tasks.

## 3. Token Management
- **Token Limits:** Be mindful of context window token limits. Implement strategies for summarizing or truncating long inputs (e.g., logs, history) before sending to the LLM.
- **Cost Optimization:** Optimize prompt length and response size to manage token costs.

## 4. Output Parsing & Validation
- **Robust Parsing:** Implement robust parsing logic for AI model outputs, especially when expecting structured formats (JSON, XML).
- **Validation:** Validate parsed outputs against expected schemas or patterns. Handle cases where the model deviates from the requested format.
- **Error Handling:** Gracefully handle and log instances where AI model output is malformed or uninterpretable.

## 5. Responsible AI & Safety
- **Bias Detection:** Continuously monitor and evaluate model outputs for potential biases.
- **Hallucination Mitigation:** Design prompts and retrieval processes to minimize hallucinations. Always ground responses in factual, retrieved data.
- **Safety Filtering:** Implement post-processing to filter or flag any unsafe, unethical, or inappropriate content generated by the AI models.
- **User Trust:** Build transparency by indicating when AI-generated content is being presented and provide options for human review.

## 6. Iterative Improvement
- **Feedback Loop:** Establish a feedback loop for model performance. Use the `Learning System` (MongoDB cases) to capture good and bad model outputs for future fine-tuning or prompt refinement.
- **Model Versioning:** Track which model version was used for each diagnostic case to enable retrospective analysis and comparison.